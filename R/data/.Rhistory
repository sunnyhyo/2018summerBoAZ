library(ggplot2)
library(dplyr)
library(gapminder)
library(gridExtra)
#################
#plot(), qplot(), ggplot() 비교하기
#plot
plot(mpg$cty, mpg$hwy)
#qplot
qplot(x = cty, y = hwy, color = cyl, data = mpg, geom = "point")
#ggplot2
ggplot(data = mpg, aes(x = cty, y = hwy))
ggplot(mpg, aes(x = cty, y = hwy)) +
geom_point(aes(color=cyl)) +
geom_smooth(method ="lm") +
coord_cartesian() +
scale_color_gradient()+
theme_bw()
mpg %>% ggplot(aes(cty, hwy)) +
geom_point(aes(color=cyl)) +
geom_smooth(method ="lm") +
coord_cartesian() +
scale_color_gradient()+
theme_bw()
if(!require(caret)) install.packages("caret"); library(caret)
if(!require(randomForest)) install.packages("randomForest"); library(randomForest)
if(!require(C50)) install.packages("C50"); library(C50)
if(!require(ROCR)) install.packages("ROCR"); library(ROCR)
if(!require(e1071)) install.packages("e1071"); library(e1071)
setwd("C:/Users/HS/Documents/GitHub/2018summerBoAZ/R")
# 1)첫번째  방법
data.url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
red.wine = read.csv(url(data.url), sep = ';')
head(red.wine)
model = lm(quality ~ alcohol, red.wine)
summary(model)
# 2)두번째 방법
set.seed(123)
idx = createDataPartition(red.wine$quality, p=.8, list=F)
data.train = red.wine[idx, ]
data.test = red.wine[-idx, ]
lin.model = train(
quality ~ .,
data=data.train,
method='lm')
y.pred = predict(lin.model, data.test)
RMSE(data.test$quality, y.pred)
R2(data.test$quality, y.pred)
data<-read.delim("Hshopping.txt", stringsAsFactors=FALSE)
head(data)
setwd("C:/Users/HS/Documents/GitHub/2018summerBoAZ/R/data")
data<-read.delim("Hshopping.txt", stringsAsFactors=FALSE)
head(data)
## 우리가 알고 싶은 변수는? = 반품여부
## C50 패키지의 경우 우리 알고싶은 변수를 Factor화 해줘야 함
data$반품여부<-factor(data$반품여부)
# set.seed() 설정을 해주면 결과값이 똑같이 나오게됨
set.seed(123)
# 데이터 나누기(sampling  train=7 : test=3)
idx<-createDataPartition(y=data$반품여부, p=0.7, list=FALSE)
data.train<-data[idx,]
data.test<-data[-idx,]
# 모델 구축
rf_model <- randomForest(반품여부 ~ . -ID, data=data.train, ntree=500, mtry=2)
rf_pred<-predict(rf_model, data.test, type="response")
confusionMatrix(rf_pred, data.test$반품여부)
# accuracy : (TP+TN) / N
(36+94)/(36+94+9+10)
# ROC curve
data.test$rf_pred_prob<-predict(rf_model, data.test, type="prob")
rf_pred2<-prediction(data.test$rf_pred_prob[,2],data.test$반품여부)
rf_model.perf1 <-performance(rf_pred2, "tpr", "fpr") # ROC-chart
plot(rf_model.perf1)
#AUC
performance(rf_pred2, "auc")@y.values[[1]] #0.5< AUC< 1 : 1에 가까울수록 좋다.
red.url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
red.url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
white.url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
red.wine = read.csv(url(red.url), sep = ';')
white.wine = read.csv(url(white.url), sep = ';')
red.wine$color = 'red'
white.wine$color = 'white'
wine = rbind(red.wine, white.wine)
set.seed(1234)
idx = createDataPartition(wine$color, p=.8, list=F)
data.train = wine[idx, ]
data.test = wine[-idx, ]
control = trainControl(method='cv', search='grid', number=5)
xgb.grid = expand.grid(
.nrounds = 100,
.eta = 0.5,
.gamma = 1,
.max_depth = c(3, 5),
.min_child_weight = 1,
.colsample_bytree = 1,
.subsample = 0.8
)
xgb.model <- train(
color ~ .,
data = data.train,
tuneGrid = xgb.grid,
trControl = control,
method = 'xgbTree'
)
xgb.grid = expand.grid(
.nrounds = 100,
.eta = 0.5,
.gamma = 1,
.max_depth = c(3, 5),
.min_child_weight = 1,
.colsample_bytree = 1,
.subsample = 0.8
)
xgb.model <- train(
color ~ .,
data = data.train,
tuneGrid = xgb.grid,
trControl = control,
method = 'xgbTree'
)
xgb_pred = predict(xgb.model, data.test)
data.test$color <- as.factor(data.test$color)
confusionMatrix(xgb_pred, data.test$color)
